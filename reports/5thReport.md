# Parser & Building an Abstract Syntax Tree
### Course: Formal Languages & Finite Automata
### Author: Ciumac Alexei FAF-212

---

## Theory

### Parsing
Parsing is the process of analyzing a sequence of tokens (typically generated by a lexer) according to a formal grammar or syntax rules to determine its structure and meaning. It involves breaking down the input into a hierarchical structure represented by a parse tree or an abstract syntax tree (AST).

During parsing, the input is typically validated against a grammar, which defines the valid syntax rules for a given language. The grammar specifies how the tokens can be combined to form valid expressions, statements, or other language constructs.

The parser applies parsing algorithms, such as _top-down_ parsing or _bottom-up_ parsing, to recognize the input's structure based on the grammar rules. It constructs a parse tree or _AST_ that represents the hierarchical structure of the input, where each node corresponds to a grammar rule and its children represent the components of that rule.

Parsing plays a crucial role in various areas of computer science, including compiler design, programming language processing, natural language processing, and data validation. It allows for the analysis, interpretation, and manipulation of structured data according to a defined syntax.

### AST
AST stands for Abstract Syntax Tree. It is a hierarchical representation of the structure of source code or expressions in a programming language. An AST abstracts away the low-level details of the source code and focuses on its syntactic structure and semantics.

An AST is typically generated during the parsing phase of a compiler or interpreter. It captures the essential elements of the source code, such as statements, expressions, function calls, conditionals, loops, and more, in a tree-like structure. Each node in the tree represents a specific language construct, and the relationships between nodes reflect the syntactic relationships defined by the programming language's grammar.

The AST provides a more abstract view of the source code, separating it from the textual representation. It helps in analyzing and transforming the code during compilation or interpretation. The AST can be traversed, analyzed, and modified by various stages of a compiler or interpreter to perform tasks such as type checking, optimization, code generation, or static analysis.

By representing the source code in a structured and hierarchical manner, the AST enables tools and systems to reason about the code's structure, perform transformations, and extract information for various programming language-related tasks. It serves as an intermediate representation that facilitates efficient and accurate processing of the source code.

The main advantages of using an AST include:

1. Easier processing: The hierarchical structure of the AST simplifies the processing of the source code during later stages, such as semantic analysis, code optimization, or code generation.

2. Simplified code transformation: The AST enables simple and efficient manipulation of the code structure, such as code optimizations, transformations, or refactoring.

3. Language-agnostic representation: An AST can be generated for any programming language, making it a flexible and reusable representation for different language processors.

4. Improved error reporting: With an AST, it is easier to detect and report syntax and semantic errors, as well as to provide meaningful error messages, as the hierarchical structure provides context to the code constructs.

## Objectives:

1. Get familiar with parsing, what it is and how it can be programmed [1].
2. Get familiar with the concept of AST [2].
3. In addition to what has been done in the 3rd lab work do the following:
   1. In case you didn't have a type that denotes the possible types of tokens you need to:
      1. Have a type __*TokenType*__ (like an enum) that can be used in the lexical analysis to categorize the tokens. 
      2. Please use regular expressions to identify the type of the token.
   2. Implement the necessary data structures for an AST that could be used for the text you have processed in the 3rd lab work.
   3. Implement a simple parser program that could extract the syntactic information from the input text.

## Implementation description

First of all I changed the lexer class. The first version uses a while loop and a for loop to iterate through the input string and match tokens using regular expressions. In the second version, the input source code is split into tokens using the split method, and a single while loop is used to iterate through the tokens.

The second version introduces different categories for tokens, such as "DECLARATION", "INTEGER", "IDENTIFIER", "OPERATOR", and "SEPARATOR". Each token is appended to the tokens list as a list containing the token category and the token value.

The second version includes additional checks for special cases, such as checking if a token ends with a semicolon and removing the semicolon before appending the token to the tokens list.

```
import re


class Lexer:
    def __init__(self, src_code):
        self.src_code = src_code

    def tokenize(self):
        # This list will keep all tokens
        tokens = []

        # Splitting the source code into "tokens"
        src_code = self.src_code.split()

        t_index = 0

        # Iterating through "tokens" and dividing into categories
        while t_index < len(src_code):
            token = src_code[t_index]

            # Recognise variable declaration
            if token == "var":
                tokens.append(['DECLARATION', token])

            # Recognise an integer
            elif re.match('[0-9]', token):
                if token[-1] == ";":
                    tokens.append(['INTEGER', token[:-1]])
                else:
                    tokens.append(['INTEGER', token])

            # Recognise a word
            elif re.match('[a-z]', token) or re.match('[A-Z]', token):
                if token[-1] == ";":
                    tokens.append(['IDENTIFIER', token[:-1]])
                else:
                    tokens.append(['IDENTIFIER', token])

            # Recognise an operation
            elif token in "+-=/*":
                tokens.append(['OPERATOR', token])

            # Recognise the separator, for this case also end of declaration
            if token[-1] == ";":
                tokens.append(['SEPARATOR', token[-1]])

            t_index += 1
        return tokens
```

**Abstract Syntax Tree (AST) Node Classes:**
The implementation starts by defining a base Node class and several subclasses representing different types of nodes in the AST. Each subclass has an **___init___** method for initializing the node with the required attributes.
```
class Node:
    pass

class VariableDeclaration(Node):
    def __init__(self, identifier, value):
        self.identifier = identifier
        self.value = value

# Other node classes: Assignment, Identifier, Integer, String, Operator
```
**Parser Class:**
A Parser class is defined that takes a list of tokens generated by a lexer and converts them into a list of AST nodes. The class provides functions for parsing variable declarations **_parse_var_declaration_** and building the AST **_parse_**.
```
class Parser(object):
    def __init__(self, tokens):
        self.tokens = tokens
        self.token_index = 0

    def parse(self):
        ast_nodes = []
        # Parse the tokens and build the AST nodes

    def parse_var_declaration(self, tokens):
        # Parse a variable declaration and create an AST node for it

```

**Parsing Variable Declarations:**
The **_parse_var_declaration_** function iterates through the tokens in a variable declaration, creating the necessary _Identifier_, _Operator_, and _Value_ nodes (either _Integer_ or _String_). It then constructs a _VariableDeclaration_ node using these nodes.
```
def parse_var_declaration(self, tokens):
    tokens_verified = 0
    identifier = None
    operator = None
    value = None

    for token in range(0, len(tokens)):
        token_type = tokens[tokens_verified][0]
        token_value = tokens[tokens_verified][1]

        if token == 1 and token_type == 'IDENTIFIER':
            identifier = Identifier(token_value)
        elif token == 2 and token_type == 'OPERATOR':
            operator = Operator(token_value)
        elif token == 3 and token_type in ['IDENTIFIER', 'INTEGER', 'STRING']:
            # Create a Value node based on the token_type

    variable_declaration = VariableDeclaration(identifier, value)
    return variable_declaration

```

**_Building the AST:_**
The main parse function iterates through the tokens and calls **_parse_var_declaration_** whenever it encounters a variable declaration token. It then adds the returned _VariableDeclaration_ node to the ast_nodes list.
```
def parse(self):
    ast_nodes = []

    while self.token_index < len(self.tokens):
        token_type = self.tokens[self.token_index][0]
        token_value = self.tokens[self.token_index][1]

        if token_type == 'DECLARATION' and token_value == 'var':
            var_decl_node = self.parse_var_declaration(self.tokens[self.token_index:len(self.tokens)])
            ast_nodes.append(var_decl_node)

        self.token_index += 1

    return ast_nodes
```
## Results
For the provided source code, the AST will be the following one:

```var number = 256;```

```
VariableDeclaration
Identifier: number
Value (Integer): 256
```
## Conclusion
To summ up, in this laboratory work I learned about Parsing and Abstract Syntax tree.
Parsing, or syntactic analysis, is a fundamental step in the process of processing programming languages. It checks the conformity of the input program to the syntax rules of the language, creates a hierarchical structure (usually an AST), and allows for further processing in the compiler or interpreter pipeline. 
An Abstract Syntax Tree (AST) is a hierarchical representation of the structure of a program's source code, abstracting away syntactic details. It captures the essential relationships and organization of code constructs, facilitating analysis and transformation tasks in the language processing pipeline.